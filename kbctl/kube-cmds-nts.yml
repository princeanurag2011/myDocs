
KUBE-VERSION-history:
   - https://blog.risingstack.com/the-history-of-kubernetes/
   - https://www.thoughtworks.com/insights/blog/kubernetes-exciting-future-developers-and-infrastructure-engineering-0 
   - initial release JUNE 7, 2014
   - v1.0 RELEASED IN 2015 JULY 21
   - minikube 2016 july
   - KOPS -  PRODUCTION GRADE kubernetes cluster sep 2016
   - Windows server support 2016 dec
   - v1.6 nov 2017 
   - GKE 1.10 2018 may 21  general available
   - v 1.13 RELEASED IN 2018 DEC
   - v ALPHA - EXPERIMENTAL (NEW FEAUTUREs ARE DISABLED BY DEFAULT )
   - v BETA  -  TESTED VERSION (NEW FEAUTUREs ARE ENABLED BY DEFAULT ) 
   - v 1.14 - STABLE version production versions

kube-apiserver , control mgr, 	kube scheduler,  kubelet, kue-proxy , kubectl  all have 'same version by deafult'
coreDNS and ETCD Cluster have 'different versions'

================================================================== 
                          Namespaces
==================================================================

NameSpaces:
  kubeadm creates 3 namespaces by default:
    - kube-system
    - default
    - kube-public
  
# namespaces
#To see list of Namespaces:
  kubectl get namespaces
  
# To create a namespace (say dev) 
 kubectl create namespace dev
 
# To create namespace usign YML file:
kubectl create -f namespace-dev.yml #check the yaml file below

-------namespace-dev.yml-------------------------
  --- 
  #namespace-dev.yml
  apiVersion: v1
  kind: Namespace
  metadata:
    name: dev
-------------------------------    
# To access the services with in the same namespace (default).
# We can refer to them directly by using their Names
  example: mysql.connect(db-service)
# if different namespace (dev) in the same cluster
 example: mysql.connect(db-service.dev.svc.cluster.local)


================Namespace-quotas==============================
 quota limits: Set resource limits to namespace
 
 # To create namespace using YML file:
  --- 
  #namespace-dev-with-quota-limits.yml
  apiVersion: v1
  kind: Namespace
  metadata:
    name: dev
  spec:
    hard:
      pods: "10"
      requests.cpu: "4"
      requests.memory: 5Gi
      limits.cpu: "10"
      limits.memory: "10Gi"
 ==============================================================
====================== ====================== ====================== 
                       BASIC POD CREATION
====================== ====================== ====================== 
#To set the default Namespace (say dev)
  kubectl config set-context $(kubectl config current-context)  --namespace=dev
# YAML IN KUBERENETES BASIC
# EVERY YAML FILE SHOULD HAVE FOLLOWING
 ---
 apiVersion:
 kind:
 metadata:
 
 spec:

=========================================================
--- pod-definition.yml------and pod creation-----------------------
 apiVersion: v1
 kind: Pod 
 #here kind and api version can be 
                  #Pod  and apiVersion is v1
                  #Service and apiVersion is v1
                  #ReplicaSet and apiVersion is apps/v1
                  #Deployment and apiVersion is apps/v1
 metadata: 
    # this is the name assigned to pod ; 
    # if replicas are created additional hashcode will be added to same name
   name: myapp-pod
   #if we don't provide namespace tag here ,  then by default pods gets created in 'default' namespace
   #namespace: dev 
   labels:
     app: myapp
     type: front-end
    
 spec:
   containers:
     - name: nginx-controller
       image: nginx
       ports:
        - containerPort:80
   nodeSelector: #to schedule pods on particular node we use node selector with labels
     availableZone: 1a
     
------------commands to create pods--------------------------
#By default pods gets created in default namespace
#To create pod using above file in default namespace
 kubectl create -f pod-definition.yml
#note: here -f is used to provide file as input
#or
 kubectl run myapp-pod --image=nginx --generator=run-pod/v1

#To create pod in a specific namespace (say dev)
 kubectl create -f pod-definition.yml --namespace=dev

#To see list of pods with high level information
 kubectl get pods

#To see list of pods in a specific namespace (say dev)
 kubectl get pods --namespace=dev
 
#To see list of pods with detailed information
 kubectl get pods -o wide

#To see list of pods with sort order 
 kubectl get pods -o wide --sort-by="{.spec.nodename}"

#To see detailed information of pods like labels, containers, created time, events etc.
 kubectl describe pods myapp-pod

#To remove the pod by name
 kubectl delete pods <pod-name>
 
#To remove all the pods 
 kubectl delete pods --all

# Delete pods and services with label name=myLabel.
 kubectl delete pods,services -l name=myLabel
 #ex: kubectl delete pods,services -l env=dev
#Edit a POD
#Remember, you CANNOT edit specifications of an existing POD other than the below.
#spec.containers[*].image
#spec.initContainers[*].image
#spec.activeDeadlineSeconds
#spec.tolerations
 #Run the kubectl edit pod <pod name> command.  This will open the pod specification in an editor (vi editor). 
 #Then edit the required properties. When you try to save it, you will be denied. 
 #This is because you are attempting to edit a field on the pod that is not editable.
 # A copy of the file with your changes is saved in a temporary location /tmp/pod093e2e.yaml
appConfiguration:

#Example using hostPath volumes
#If you create a node pool with three local SSDs, the host OS mounts the disks at 
#/mnt/disks/ssd0, /mnt/disks/ssd1 and /mnt/disks/ssd2. 
#Your Kubernetes containers access the disks using the hostPath parameter 
#in defined in your object's configuration file.

#This example Pod configuration file references a local SSD: /mnt/disks/ssd0:
----------------------------pod-def.yml---- with hosted volumes-----------
 apiVersion: v1
 kind: Pod
 metadata:
   name: "test-ssd"
 spec:
   containers:
   - name: "shell"
     image: "ubuntu:14.04"
     command: ["/bin/sh", "-c"]
     args: ["echo 'hello world' > /test-ssd/test.txt && sleep 1 && cat /test-ssd/test.txt"]
     volumeMounts:
     - mountPath: "/test-ssd/"
       name: "test-ssd"
   volumes:
   - name: "test-ssd"
     hostPath:
       path: "/mnt/disks/ssd0"
   nodeSelector:
     cloud.google.com/gke-local-ssd: "true"

---------------------------------------------------------------
====================== ====================== ====================== 
                       LABELS SELECTORS AND ANNOTATIONS
====================== ====================== ====================== 

#To see list of pods with labels 
 kubectl get pods -o wide --show-labels


#To see list of nodes with labels 
 kubectl get nodes -o wide --show-labels
 
 
#To assign labels to nodes
 kubectl label nodes <node-name> <label-name>=<label-value>
 #ex: kubectl label nodes ip-123.23.123.12 availabiltyZone=1a
 
#To remove the label assignd to pod
 kubectl label pods <pod-name> <label-name> -
 #ex: to remove the label availabiltyZone=1a then
 # kubectl label pods myapp-pod availabiltyZone -
 
#To see the list of pods using Selector
 kubectl get pods --selector <label-name>=<label-value>
#ex: kubectl get pods --selector availabiltyZone=1a

====================== ====================== ====================== 
                       REPLICATION CONTROLLER
====================== ====================== ====================== 



---------------rc-definition.yml----creating replicas and replication conttrollers-----------
#note: the appVersion should be v1 for ReplicationController
 apiVersion: v1
 kind: ReplicationController
 metadata: 
   name: webserver-rc
   labels:
     app: webserver
     type: front-end
 spec:
   template:  
     metadata: 
       name: webserver-pod
       labels:
         app: webserver
         type: front-end    
     spec:
       container:
         - name: nginx-controller
           image: nginx  
   replicas: 3
   selector:
     env: dev
--------commands to create replication controller------------------------------------

#To create replication COntroller using above file.
 kubectl create -f rc-definition.yml

#To update the replication controller make changes in it and then apply using below command
 kubectl apply -f rc-definition.yml

#To see created replication controllers
 kubectl get replicationcontroller
#or
 kubectl get rc
#note: we can see desired-current-ready status

#To see detailed info replication controller like status events etc..
 kubectl describe rc


#To see no of pods created after running replicaiton controller
 kubectl get pods -o wide
#NOte: we can see three pods if we used above to file to create replicas

#To remove the replicationcontroller  and also pods underlying
 kubectl delete replicationcontroller webserver-rc
 
----------------------------------------------------- 

====================== ====================== ====================== 
                        REPLICA SET
====================== ====================== ====================== 


---------------ReplicaSet-definition.yml---------------
#note: the appVersion should be apps/v1 for ReplicationController
#if apiVersion is v1 , we can get error unable to recognize
 apiVersion: apps/v1
 kind: ReplicaSet
 metadata: 
   name: webserver-replicaset
   labels:
     app: webserver
     type: front-end
    
 spec:
   template:  
     metadata: 
       name: webserver-pod
       labels:
         app: webserver
         type: front-end    
     spec:
       container:
         - name: nginx-controller
           image: nginx  
   replicas: 3
   selector: #for repicaset selector is used as additonal feild
     matchLabels:
       type: front-end
----------------commands to create replica sets----------------------------

#To create replicaset using above file.
 kubectl create -f replicaset-definition.yml
#To edit the replicaSet(say rs name: webserver-replicaset)
 kubectl edit rs <replicaset-name>
 #ex: kubectl edit rs webserver-replicaset

#To export replicaset to yaml file
 kubectl get rs webserver-replicaset -o yaml > rs.yml
#-----------------------------------------------
#To increase the no. of replicas to 6 
#change the replicas to 6 in the above file
#then run the below command
 kubectl replace -f replicaset-definition.yml

#or  we can do using tags for above file
 kubectl scale --replicas=6 -f replicaset-definition.yml

#or  we can do using type and name format
 kubectl scale --replicas=6 replicaset myapp-replicaset
#-----------------------------------------------------

#To see created replicaset
 kubectl get replicaset
#note: we can see desired-current-ready status

#To see no of pods created after running replicaiton controller
 kubectl get pods -o wide
#NOte: we can see three pods if we used above to file to create replicas

#To remove the replicaset and also pods underlying
 kubectl delete replicaset webserver-replicaset

-----------------------------------------------------------------

====================== ====================== ====================== 
                             SERVICES
====================== ====================== ====================== 

---------------------Expose-service-nginx.yml------external-load-balancer-----------------------------
 apiVersion: v1
 kind: Service
 metadata:
   creationTimestamp: "2019-12-21T08:52:57Z"
   finalizers:
   - service.kubernetes.io/load-balancer-cleanup
   labels:
     app: nginx-1
   name: nginx-1-service #service name is created with this name
   namespace: default
   resourceVersion: "15225"
   selfLink: /api/v1/namespaces/default/services/nginx-1-service
   uid: c9bacdea-513d-4cf5-9d99-3d21ed77b8e9
 spec:
   clusterIP: 10.0.1.163
   externalTrafficPolicy: Cluster
   ports:
   - nodePort: 31672
     port: 80
     protocol: TCP
     targetPort: 80
   selector:
     app: nginx-1
     env: dev
   sessionAffinity: None
   type: LoadBalancer #for load balancing
   #type: nodePort #for node port
  
 status:
  loadBalancer:
    ingress:
    - ip: 35.193.18.231
--------------------------commands to create services-------------------------------
 services are used to access the application from:
    - outside of the cluster:
         <node-ip>:<nodePort>
    - within the cluster:
         <cluster-ip>
    - load balancer:
        - provided by cloud services
 
#To create service from service-nginx.yml file
 kubectl create -f service-nginx.yml
 
#To see the created service 
 kubectl get svc
 
#To see the detialsed info of services
 kubectl describe svc <service-name> 
#ex:kubectl describe svc nginx-1-service

#TO delete service 
 kubectl delete svc <service-name>
--------------------------------------------------------------------

================================================================== 
                              DEPLOYMENTS
================================================================== 

----------------nginx-deploy.yml-----no.of replicas=1
 apiVersion: apps/v1
 kind: Deployment
 metadata:
   annotations:
     deployment.kubernetes.io/revision: "1"
   creationTimestamp: "2019-12-21T08:32:52Z"
   generation: 2
   labels:
     app: nginx-1
   name: nginx-1
   namespace: default
   resourceVersion: "11734"
   selfLink: /apis/apps/v1/namespaces/default/deployments/nginx-1
   uid: dd0f6cf8-9a1d-4f7c-bfd7-dbb4bf8e9120
 spec:
   progressDeadlineSeconds: 600
   replicas: 1
   revisionHistoryLimit: 10
   selector:
     matchLabels:
       app: nginx-1
   strategy:
     rollingUpdate:
       maxSurge: 25%
       maxUnavailable: 25%
     type: RollingUpdate
   template:
     metadata:
       creationTimestamp: null
       labels:
         app: nginx-1
     spec:
       containers:
       - image: nginx:latest
         imagePullPolicy: Always
         name: nginx
         resources: {}
         terminationMessagePath: /dev/termination-log
         terminationMessagePolicy: File
       dnsPolicy: ClusterFirst
       restartPolicy: Always # incase the containers stops execution of process inside it, it will try to restart always
     # restartPolicy: Never # incase the containers stops execution of process inside it, it will not restart
       schedulerName: default-scheduler
       securityContext: {}
       terminationGracePeriodSeconds: 30
 status:
   availableReplicas: 1
   conditions:
   - lastTransitionTime: "2019-12-21T08:32:54Z"
     lastUpdateTime: "2019-12-21T08:32:54Z"
     message: Deployment has minimum availability.
     reason: MinimumReplicasAvailable
     status: "True"
     type: Available
   - lastTransitionTime: "2019-12-21T08:32:52Z"
     lastUpdateTime: "2019-12-21T08:32:54Z"
     message: ReplicaSet "nginx-1-74c64df7b" has successfully progressed.
     reason: NewReplicaSetAvailable
     status: "True"
     type: Progressing
   observedGeneration: 2
   readyReplicas: 1
   replicas: 1
   updatedReplicas: 1



--------------------------------------------------------------------
-------------nginx-deploy.yml-----no.of replicas=3

 apiVersion: apps/v1
 kind: Deployment
 metadata:
   annotations:
     deployment.kubernetes.io/revision: "1"
   creationTimestamp: "2019-12-21T08:32:52Z"
   generation: 3
   labels:
     app: nginx-1
   name: nginx-1
   namespace: default
   resourceVersion: "17905"
   selfLink: /apis/apps/v1/namespaces/default/deployments/nginx-1
   uid: dd0f6cf8-9a1d-4f7c-bfd7-dbb4bf8e9120
 spec:
   progressDeadlineSeconds: 600
   replicas: 3
   revisionHistoryLimit: 10
   selector:
     matchLabels:
       app: nginx-1
   strategy:
     type: RollingUpdate
  #  rollingUpdate:
   #   maxSurge: 1
   #   maxUnavailable: 0
     rollingUpdate:
       maxSurge: 25%
       maxUnavailable: 25%
   template:
     metadata:
       creationTimestamp: null
       labels:
         app: nginx-1
     spec:
       containers:
       - image: nginx:latest
         imagePullPolicy: Always
         name: nginx
         resources: {}
         terminationMessagePath: /dev/termination-log
         terminationMessagePolicy: File
       dnsPolicy: ClusterFirst
       restartPolicy: Always
       schedulerName: default-scheduler
       securityContext: {}
       terminationGracePeriodSeconds: 30
 status:
   availableReplicas: 3
   conditions:
   - lastTransitionTime: "2019-12-21T08:32:52Z"
     lastUpdateTime: "2019-12-21T08:32:54Z"
     message: ReplicaSet "nginx-1-74c64df7b" has successfully progressed.
     reason: NewReplicaSetAvailable
     status: "True"
     type: Progressing
   - lastTransitionTime: "2019-12-21T09:05:38Z"
     lastUpdateTime: "2019-12-21T09:05:38Z"
     message: Deployment has minimum availability.
     reason: MinimumReplicasAvailable
     status: "True"
     type: Available
   observedGeneration: 3
   readyReplicas: 3
   replicas: 3
   updatedReplicas: 3

----------------------KUBECTL DEPLOY COMMANDS ------------

 Deployments has the strategy of deployments:
    - recreate strategy:
        Terminate old version and release new versions. ALL happens at once
    - rolling Updates:
        release the updates in incremental fashion 
        one batch after other
        once new batch is formed one by one. old batch gets terminated one by one
    - Other strategy:
        - BLUE_GREEN:
            release new version along with old version and switching traffic    
        - Canary:
            release a new version to a subset of users(say  25% users), 
            then proceed to a full rollout(remaning %75 )
        - A/B Testing:
            Routing subset of users to a new fucntionality under specific verison 
            based on certain decisions 
        

 Reasons for deployment failures :
   - insufficinet permission
   - insufficinet quota of resources
   - Image pull error
   - Application run time misconfigurations
   - Readiness probe failures
   
# To create deployment strategy
 kubectl create -f nginx-deploy.yml
 
# To see the status of the deployment 
 kubectl get deploy <deployment-name> 
 # For deployment name -> check in nginx-deploy.yml above look for name in the metadata section
 #Ex: kubectl get deploy nginx-1

# To update the no. of replicas or version no make changes in the above yml file and use below command
 kubectl apply -f nginx-deploy.yml
 
# To update the deployment and keep a track record of every deployment 
# this is done beacuase in case of any issue with deployed version we can rollback immediately
 kubectl apply -f nginx-deploy.yml --record=true
 
# To check rolling updates deployments status
 kubectl rollout status deploy <deployment-name>
 
# To check the no of replicas sets created
 kubectl get rs <replicaset-name> -o wide

# To check the history i.e REVISION NO. and CHANGE CAUSE of roll out deployments
 kubectl rollout history deploy <deployment-name>
 
# To check the detailed history of revision
 kubectl rollout history deploy <deployment-name> --revision=1
 
#To add our own message to the CHANGE CAUSE after applying changes using below command
 kubectl apply -f nginx-deploy.yml --record=true
 #run the next command to update change cause we can do this using 'kubectl annotate'
 kubectl annotate deployment.v1.apps/<deployment-name> kubernetes.io/change-cause="add your message here"
  
# To roll back the deployment to immediate pevious version 
 kubectl rollout undo deploy <deployment-name>

# To roll back the deployment to specific version or revision (say 1) 
 kubectl rollout undo deploy <deployment-name> --to-revision=1

# TO pause the deployments
 kubectl rollout pause deploy <deployment-name>
 
# TO resume the deployments
 kubectl rollout resume deploy <deployment-name>
 
#note:
 ####Deployment revision is triggered only when the 'rollout' is triggered
 ####In the deployment revision/version history only changes made to template like
 ####changing version no. and labels etc., then only deployment revision is created 
 ####other updates like scaling the deployment don't create  deployment revision.
 
--------------------------------------------------------------------------






=======================================================================
                         JOBS AND CRON JOBS
=======================================================================

JOBS:

-----------------------------job-def.yaml----------------------------
 apiVersion: batch/v1
 kind: Job
 metadata: 
    name: sample-addition-job
 spec:
  backoffLimit: 4 #to specify the number of retries before considering a Job as failed. 
  #The back-off limit is set by default to 6
  completions: 3 #To make sure 3 job are in completed status
  #if parallelism is not used then it ceates pod sequentially
  parallelism: 3 #To make sure 3 pods are created at a time and ensures the status is completed
   #if  1/3 failed to complete the job , then it will not create 3 next time only 1 pod is created to get 3 completed.
  template:
    spec:
       contianers:
         - name: math-add
           image: ubuntu
           command: ['expr', '3', '+', '2']
       restartPolicy: Never
---------------------------------------------------------
#To create a job 
 kubectl create -f job-def.yaml

#To see the list of jobs
 kubectl get jobs

#To see the list of pods created by job
 kubectl get pods #look at the status 

#To see the output of the executed job we can use logs
 kubectl logs <job-pod-name>
 #ex: kubectl logs sample-addition-job

#To delete the jobs 
 kubectl delete job <job-name>
 #ex: kubectl delete job sample-addition-job
---------------------------------------------------------------

CRON-JOBS:
-----------------------------cronjob-def.yaml----------------------------
 apiVersion: batch/v1beta1
 kind: CronJob
 metadata: 
    name: sample-addition-job
 spec:
    schedule:   "*/1 * * * * " #check cat /etc/crontab #to view the format
      #  #format of CRONETAB:
      #  <min> <hour> <date of month> <month> <day of week> <username> <write command here>
    jobTemplate:
       spec:
        completions: 3 #To make sure 3 job are in completed status
        #if parallelism is not used then it ceates pod sequentially
        parallelism: 3 #To make sure 3 pods are created at a time and ensures the status is completed
         #if  1/3 failed to complete the job , then it will not create 3 next time only 1 pod is created to get 3 completed.
        template:
          spec:
            contianers:
              - name: math-add
                image: ubuntu
                command: ['expr', '3', '+', '2']
            restartPolicy: Never
---------------------------------------------------------

#To create a cronjob 
 kubectl create -f cronjob-def.yaml

#To see the list of jobs
 kubectl get cronjobs

=======================================================================
                          APP LIFE CYCLE MANAGEMENT
=======================================================================

   - Configuring Command and Arguments on applications:
        ---
        apiVersion:
        kind:
        metadata:
        spec:
          contianers:
            - name:
               image:
               command: ["sleep2.0"]  #similar to ENTRYPOINT
               args: ["10"] #similar to CMD

   - Configuring Environment Variables:
        #Key value Pair:
         ---
          #pod-def.yaml
         apiVersion:
         kind:
         metadata:
         spec:
           contianers:
             - name:
               image:
               env:
                  - name: APP_COLOR
                    value: pink
        #To see the list of configMaps:
           kubectl get configmaps
           
        # config Maps: from literals
            kubectl create configmap <confg-name> --from-literal=<key>=<value>
                ex:  kubectl create configmap my-config --from-literal=APP_COLOR=pink \
                																	   --from-literal=APP_MOD=prod
                																	   
               # config Maps: from file
                kubectl create configmap <confg-name> --from-file=<path-to-file>
                #ex:  kubectl create configmap my-config --from-file=app_config.properties
                 --- 
                 #app_config.properties
                 APP_COLOR=pink
                 APP_MOD=prod
                 -----------------------
                 ---
                 #pod-def.yaml
                  ---
		          apiVersion:
		          kind:
		          metadata:
		          spec:
		            contianers:
		             - name:
		                image:
		             #  env:
		             #  - name: APP_COLOR
		             #     valueFrom:
		             #       ConfigMapKeyRef:
		             #         name: app-config
		             #         key: APP_COLOR
		                      
		                envFrom:
		                 - configMapRef:
		                         name: app-config
        ---
        #Config-Maps.yml:
         
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: app-config
        data:
	         APP_COLOR: pink
	         APP_MOD: prod  
        -----------------------       
   - Configuring Secrets:
    
    #To see list of secrets:
      kubectl get secrets
         
    #To create Secret from literals
       kubectl create secret generic <secretName>  --from-literals=<key>=<value>
      ex: kubectl create secret generic appsecret  --from-literals=DB_HOST=mysql
      
      #To create Secret from file
       kubectl create secret generic <secretName>  --from-file=<filePath>
      ex: kubectl create secret generic appsecret  --from-file=app_secret.properties
   
    #To create from yaml file:
    #Config-secret.yml:
       ---  
        apiVersion: v1
        kind: Secret
        metadata:
          name: appsecret
        data:
	         DB_HOST: mysql   
	         #DB_HOST=98sdu89suv9su===  to encrypt this 'mysql'   ->  echo -n mysql | base64
               # to decrypt this 'mysql'  ->  echo -n mysql | base64 --decode   
      -----------------------------------
       kubectl create -f Config-secret.yml
     
     #To create pod from yaml file using secrets:
       
       ---
       #pod-def.yaml
         apiVersion:
         kind: Pod
         metadata:
         spec:
           contianers:
            - name:
               image:
               #can use anyone of the three below envFrom or env or volumes
               envFrom:
                 - secretRef:
                       name: appsecret
               env:
                 - name: DB_HOST
                    	valueFrom:
                    	  secretKeyRef:
                    	     name: appsecret
                    	     key: DB_HOST
               volumes:
                 - name: app-secret-volume
                    secret: 
                      secretName: appsecret
                      #note: each attribute is created as file in the volumes 
                       # vi /opt/app-secret-volume/DB_HOST
MultiContainerPods:
   In a multi-container pod, each container is expected to run a process that stays alive as long as the POD's lifecycle.
   For example in the multi-container pod that we talked about earlier that has a web application and logging agent, 
   both the containers are expected to stay alive at all times. The process running in the log agent container is 
   expected to stay alive as long as the web application is running. If any of them fails, the POD restarts.
   
InitContainers:
 -  But at times you may want to run a process that runs to completion in a container. 
    For example a process that pulls a code or binary from a repository that will be used by the main web application. 
    That is a task that will be run only  one time when the pod is first created. 
    Or a process that waits  for an external service or database to be up before the actual application starts. 
    That's where initContainers comes in.
    
   - You can configure multiple such initContainers as well, like how we did for multi-pod containers. 
      In that case each init container is run one at a time in sequential order.If any of the initContainers fail to complete,
      Kubernetes restarts the Pod repeatedly until the Init Container succeeds.
---
 apiVersion: v1
 kind: Pod
 metadata:
   name: myapp-pod
   labels:
    app: myapp
 spec:
   containers:
    - name: myapp-container
       image: busybox:1.28
       command: ['sh', '-c', 'echo The app is running! && sleep 3600']
   initContainers:
    - name: init-myservice
       image: busybox:1.28
       command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
    - name: init-mydb
       image: busybox:1.28
       command: ['sh', '-c', 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;']
----------------------------------------------------------


=======================================================================
                      			Scheduling 
=======================================================================
---
ManualSheduling:

   If there is no scheduler, Then pods can't be placed on nodes.
   To schedule pods manually we need to specify 'nodeName' in the definition file.
   note: we can't specify the nodeName to the running pod. We need to delete recreate deleted pod.
  ---------------------------------------------------------------------------------------------
NodeName:
 ---
 apiVersion:
 kind:
 metadata:
 
 spec:
    nodeName: 
 
---------------------------------------------------------------------------------------------
  TAINTS and TOLERATIONS:
      In CLUSTER ENV:  
        No pods are deployed/scheduled on Master default taint is applied on Master;
         To check the taints on masternode (say MasterNodeName = kubemaster)
           #kubectl describe node kubemaster | grep Taint
         
     - Taints are applied to Nodes:
            - taint-effect:
                   - NoSchedule: 
                           pods will not be placed on node
                   - PreferNoSchedule:
                           pods will not be placed on node (but no garuntee)
                   - NoExecute:
                           - pods will not be placed if pods tolerance is not compatible
                           - existing pods will be evicted that doesn't meet the tolerance 
     - Tolerations are applied to Pods:
   #To apply taints to nodes:
     kubectl taint nodes <node-name> <key>=<value>:taint-effect
     
     #ex: kubectl taint nodes node01 app=blue:NoSchedule
     #ex: kubectl taint node master node-role.kubernetes.io/master:NoSchedule #to apply taints
     #ex: kubectl taint node master node-role.kubernetes.io/master:NoSchedule- #to remove taints
 #To apply taints to pod -definition:
  ---------------------------------------------------------------------------------------------
  ---
  #pod-def-with-tolerance.yml
  apiVersion:
  kind:
  metadata:
 
  spec:
    containers:
     
    tolerations:
      - key: "app"
         operator: "Equal"
         value: "blue"
         effect: "NoSchedule"
   ---------------------------------------------------------------------------------------------

NodeSelector:
   
 #To add labels to nodes
   kubectl label nodes <node-name> <label-key>=<label-value>
    #ex: kubectl label nodes node01 size=large

 #To add nodeSelector in pod definition files:
 ---
  #pod-def-with-nodeSelector.yml
        apiVersion:
        kind:
  		  metadata:
 
        spec:
          containers:
            
          nodeSelector:
   --------------------------------------------------------------------------------------------------  
NodeAffinity:
    
    Types: (# availableNOW)
      - requiredDuringSchedulingIgnoredDuringExecution
      - preferredDuringSchedulingIgnoredDuringExecution
      - requiredDuringSchedulingRequiredDuringExecution (#may come in future)

   note: - DuringScheduling -> pod is created for the first time on that node
             - DuringExecution - > pod already running on the node   
---          
  #pod-def-with-nodeaffinity.yml
        apiVersion:
        kind:
  		  metadata:
 
        spec:
          containers:
          affinity:
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                  - matchExpression:
                       - key: size
                          operator: Exists
                        #operator: In
                        #operator: NotIn
                             values: 
                              - Large
-------------------------------------------------------------------------------------------------------
ResourceLimits:
  
  ByDefault : each pod can use resources upto 1 vCPU and 512 Mi of RAM 
  but these limits can be modified using below tags:  
 --------------------------------------------------------------------------------------------------------
 ---
  apiVersion:
  kind:
  metadata:
  
  spec:
    containers:
      - name: 
         image:
         ports:
           - containerPort: 8080
         resources:
           requests:
              memory: "1 Gi"
              cpu: 1
           limits:
              memory: "2 Gi"
              cpu: 2
-------------------------------------------------------------------     

DaemonSets:
 - it creates one replica of pod in each node.
 - these are coreDNS (say weavenet) and KubeProxy

StaticPods:
  - kubelet can run static pods without any cluster or master arrangement.
  - static pods can be deployed using yaml manifest files
  - yaml files should be placed in this location for scheduling -> /etc/kubernetes/manifests
  - kubelet.service  -> contains the 'pod-manifest-path' by default  
  - kubelet.service  -> other way to  we can add '--config=kubeconfig.yml' and 
     in the 'kubeconfig.yml' -> mention 'staticPodPath: /etc/kubernetes/manifests'
   
MultipleSchedulers:
  - multiple schedulers can be configured in master
  - kube-scheduler.service -> contains '--config=/etc/kubernetes/config/kube-scheduler.yml'
                                       '--scheduler-name=default-scheduler'
  - To create custom scheduler -> create  new service (copy content in default once)
    my-custom-.service -> change the contents  '--config=/etc/kubernetes/config/my-custom-scheduler.yml'
                                               '--scheduler-name=my-custom-scheduler'  
  
--- my-custom-scheduler.yml ---

  in the yaml file change the following under 'command:'
   --leader-elect=true
   --lock-object-namespace=lock-object-namespace
   --lock-object-name=lock-object-name

  # To use the custom scheduler to schedule pods 
   -> add 'schedulerName:' under containers:
   ---
       apiVersion:
       kind:
       metadata:
       spec:
          containers:
          schedulerName:
---------------------------------------
#To see the logs of the custom scheduler:
 kubectl logs my-custom-scheduler --namespace=kube-system
 
EVENTS:
  #To see list of events in the current Namespace:
   kubectl get events
  
  #To check the running/empty ports (default port of scheduler: 10251)
     netstat -natulp | grep <portNo>
               
=======================================================================
                      			Metrics Server
=======================================================================

MetricsServer:
   #To install metrics server:
   git clone https://github.com/kubernetes-incubator/metrics-server.git
   #then run 
   kubectl create -f deploy/1.8+/
   
   after installtion:
   
   #To see details of cluster metrics of nodes
    kubectl top node
    
    #To see the details of pods metrics 
   kubectl top pods
   
   #To see logs of conainers
   kubectl logs -f <podName>	<containerName>
   
=======================================================================
                        		CLUSTER MAINTAINANCE
 =======================================================================
 
BackupAndRestore:
  
    KUBERNETES backup and restore can be done by VALERO   formely called  ARK by HepIO
     https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster
     https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/recovery.md
     ETCD BACKUP-RESTORE:
     https://github.com/mmumshad/kubernetes-the-hard-way/blob/master/practice-questions-answers/cluster-maintenance/backup-etcd/etcd-backup-and-restore.md
  - Bakckup ResourcesConfig:
      #To export all application deployments 
         kubectl get all --all-namespaces -o yaml > all-deploy-services.yml
   - Backup ETCD Cluster: 
       #To backup etcd cluster
          ETCDCTL_API=3	ectdctl snapshot save <filename.db>
       # To check the status of snaphot version and size
          ETCDCTL_API=3	ectdctl snapshot status <filename.db>
       
    - Restore ETCD Cluster:
      #Stop the kube-api server
         service kube-apiserver stop
      #and then 
          ETCDCTL_API=3	ectdctl snapshot save <filename.db> \
          --data-dir /var/lib/etcd-from-backup
          --initial-cluster master-1=<ip-address:portNo>, master-2=<ip-address:portNo> \
          --initial-cluster-token etcd-cluster-1
          --initial-advertise-per-urls  https://${INTERNAL_IP}:2380
      #and then update following in 'etcd.service' 
         etcd.service=/usr/local/bin/etcd \\
         --initial-cluster-token etcd-cluster-1
         --data-dir=/var/lib/etcd-from-backup
      #and then restart service
        systemctl daemon-reload
        service etcd restart
      # Start the kube-api server
         service kube-apiserver stop
 
Drain: This is used to move pods from one node to other nodes. It is done in case of patching or OS Upgrades
Cordon: This is used to prevent schedulng new pods on the node.
Uncordon: This is used to resume scheduling of pods.
 
 #To drain the node
  kubectl drain node01 #node01 is the nodename 

 #To cordon the node:
  kubectl cordon node01
  
 #To uncordon the node.
   kubectl uncordon node01
 
     - KUBERNETES  will have 'same version' (by deafult)
          - kube-apiserver:
               #  (can be at X.x version below object s can be at X.x-1 version )
          - control mgr
          - kube scheduler
          - kubectl 
          
          - kubelet  #(can be at X.x+1 version) kubelet alone can run container even the control plane is down.
          - kube-proxy 
    -  will have 'different versions'
         - coreDNS
         - ETCD Cluster
    #NOTE: Kube api server supports upto 2 minor versions for upgrade

ClusterUpgradeUsingKubeadm:
     #NOTE: Kube api server supports upto 2 minor versions for upgrade
     #Upgrade should be done only to nearest minor versions only 

  # To upgrade Kubeadm and see the list of upgrades available 
    kubeadm upgrade plan
    
       
  #To check the running pods
   kubectl get pods --all-namespaces -o wide
  
Masterupgrade:
         
       #Run the command to move the pods from master node to other nodes
         kubectl drain master --ignore-daemonsets
       #Run the command 
         apt install kubeadm=1.12.0-00 
       #and then 
         kubeadm upgrade apply v1.12.0 
       #and then to upgrade the kubelet on the master node 
         apt install kubelet=1.12.0-00 
       #and then
         kubectl uncordon master 

NodeUpgrade: 
       note: NODES SHOULD BE UPGRADED ONLY ONE AT A TIME IN A SEQUENTIAL ORDER TO AVOID DOWNTIME
      #Run this command to check the node version
       kubectl get nodes -o wide
	  #Run the commands: 
	    kubectl drain node01 --ignore-daemonsets
	  #Run the commands to connect to node
        ssh node01
	  #Run the commands: 
	    apt install kubeadm=1.12.0-00 
	  #and then 
	    apt install kubelet=1.12.0-00
	  #and then logout of it
          ctrl+D
	  # and then in master 
	    kubeadm upgrade node config --kubelet-version $(kubelet --version | cut -d ' ' -f 2)
     #Run this command to check the node version
       kubectl get nodes -o wide
     #and then to enable rescheduling 
      kubectl uncordon node01
    
 =======================================================================
                               			NETWORK 
 =======================================================================
    for basic networking check ntwrk.yml file
    
-------------------------------------------------------------------------------------------------
 #to set ipaddress to two systems and connect them through switch(192.168.1.0)
   
  Classless inter-domain routing (CIDR):
  https://www.geeksforgeeks.org/classless-inter-domain-routing-cidr/
  
  #To see the network interfaces IN LINUX:
    ip link
    
  network -1:
   s-1:
     ip addr add 192.168.1.10/24 dev eth0
   s-2:
     ip addr add 192.168.1.11/24 dev eth0
     
   switch-1:
     s-1 and s-2 can connect through switch-1:  192.168.1.0
 
 network -2:
    s-3:
      ip addr add 192.168.2.10/24 dev eth0
    s-4:
      ip addr add 192.168.2.10/24 dev eth0
      
    switch-2: 
    s-3 and s-4 can connect through switch-2: 192.168.2.0
    

 router: 
  network -1 and network -2 can connect to 'router' through switch1 and 2
  
  has two ip address of networks-1,2  i.e  192.168.1.1 and 192.168.2.1
  
  These details should be present in route tables to connect two networks-1
  
  #To add these networks
    ip route add 192.168.2.0/24 via 192.168.1.1
    ip route add 192.168.1.0/24 via 192.168.2.1
    to add these ip's permanantly  if not added when we restrt these gets deleted.
      etc/network/interfaces
    
    #to add default gateway 
     ip route add default via 192.168.2.1
     #or 
     ip route add 0.0.0.0 via 192.168.2.1
    
    #enable comm b/w two interfaces eth0,1  0 - prvt and 1 - pub
    cat proc/sys/net/ipv4/ip_forward   # default value is 0 to establish comm set value to 1
    also change in /etc/sysctl.conf 
      net.ipv4.ip_forward=1
   
    #To setup hostname in linux
    cat >> /etc/hosts
    192.168.2.10 db #assigning name to ip is called as name resolution
    #now u can ping the system with  below command instead of ip
     ping db 
    
    #To add entry in DNS server 
    cat /etc/resolve.conf
    nameserver        192.168.1.100  #internal dns server
    www.facebook.com   8.8.8.8     #we dont know facebooks ip so we added dns who knows all ip's 8.8.8.8
    
    - First  ping looks for 'hosts' file and then 'resolve.conf' file for name 'db'
      This is configured in  /etc/nsswitch.conf
    
  www.facebook.com
  
   - www -> sub-domain
   - facebook -> domain-name
   - com -> top-level-domain  
     #.com - commercial, .net- network, .org - Non-profit-organization, .edu - educational
  
  #to resolve name  i.e www.facebook.com
    looks in local hosts file -> Our DNS server -> public DNS server -> .com DNS server -> facebook DNS
  
  #Record types in DNS server:
    A     web-server 192.168.0.1 # stores ipv4 address 
    AAAA  web-server 2190:219e:2901:7834:d2e8:f892:29e9:2910 # stores ipv6 address
    CNAME food.webserver.com eat.web-server.com # stores sub-domains
 
  #TO get the public ip address in linux use command 
   nslookup www.google.com
   dig www.google.com #gives more detailed info
  
  #TO see about DNS server
   https://github.com/kubernetes/dns/blob/master/docs/specification.md
   https://coredns.io/plugins/kubernetes/
    
   ---------------------------------------------------------------------------------------
 ----------------------------------------------------------------------------------------- 
 INGRESS:
   - It is service which acts as a load balancer along with SSL. 
   - in Kubernetes to implemtn it we need to deploy the nginx pod and run that service.
   
  -------------------NGINX INGRESS CONTROLLER ----------------------------
   apiVersion: extensions/v1beta1
   kind: Deployment
   metadata:
     name: nginx-ingress-controller
   spec:
     replicas: 1
     selector: 
        matchLabels:
           name: nginx-ingress
    template: 
       metadata: 
          labels: 
             name: nginx-ingress
       spec:
          containers:
               - name:  nginx-ingress-controller
                  image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0
          args:
             - /nginx-ingress-controller
             #- --configmap=$(POD_NAMESPACE)/nginx-configuration
          env:
            - name: POD_NAME
              valueFrom:
                 fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
               valueFrom:
                 fieldRef:
                   fieldPath: meatada.namespace
          ports:
           - name: http
             containerPort: 80
           - name: https
             containerPort: 443
    ------------------------------------------------------------------------------
  ----------------------nginx-ingress-service-----------------------------------
   apiVersion: v1
   kind: Service
   metadata:
      name: nginx-ingress-service
      labels:
         name: nginx-ingress-service
   spec:
     type: NodePort
     selector:
       matchLabels:
          name:  nginx-ingress
     ports:
       - port: 80
          targetPort: 80
          protocol: 	TCP
          name: http
       - port: 443
          targetPort: 443
          protocol: TCP
          name: https
     ----------------------------------------------------------------------     
  ----------------------------configmap------------------------------
   apiVersion: v1
   kind: ConfigMap
   metadata: 
     name: nginx-configuration
   -------------------------------------------------------------  
  ----------------------------Auth------------------------------
   apiVersion: v1
   kind: ServiceAccount
   metadata: 
     name: nginx-configuration
   -------------------------------------------------------------  
 IngressRules: check the below yml files
   - Single backend Service
   - multiple backendservcies based on url 
   - multiple backend services based on hostname
   
 ----------------Ingress-wear.yml-------for single backend service------------------
  apiVersion: extensions/v1beta1
  kind: Ingress
  metadata: 
    name: ingress-wear
  spec: 
    backend:
       servicename: wear-service 
       servicePort: 80
  ------------------------------------------  
  
 -----------------Ingress-wear.yml-------for multiple backend service with url------------------
  apiVersion: extensions/v1beta1
  kind: Ingress
  metadata: 
    name: ingress-wear-watch
  spec: 
    rules: 
     - http:
         - path: /wear
            backend: 
               serviceName: wear-service
               servicePort: 80
         - path	: /watch
            backend:
               serviceName: watch-service
               servicePort: 80 
    
   ------------------------------------------  

------------------Ingress-wear.yml----for multiple backend service with hostname--------
  apiVersion: extensions/v1beta1
  kind: Ingress
  metadata: 
    name: ingress-wear-watch
  spec: 
    rules: 
     - host: wear.my-online-store.com
        http:
         - path: 
            backend: 
               serviceName: wear-service
               servicePort: 80
     - host: watch.my-online-store.com
        http:
         - path: 
            backend: 
               serviceName: watch-service
               servicePort: 80
  ------------------------------------------ 
#To see detailed info Ingress..
  kubectl describe ingress ingress-wear-watch

-----------------ingress-rewrite.yaml--------------------------------------
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test-ingress
  namespace: critical-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path: /pay
        backend:
          serviceName: pay-service
          servicePort: 8282
-------------------------------------------------------
-------------------------------------------------------


 =======================================================================
                          STORAGE VOLUMES
  =======================================================================
  Volumes and Mounts:
     
    -------------------------pod-with-volume.yaml-----------------------
     apiVersion: v1
     kind: Pod
     metadata:
       name: random-number-generator
     spec:
       containers:
         - name: alpine
            image: alpine
            command: ["/bin/sh", "-c"]
            args: [ "shuf -i  0-100 -n 1 >> /opt/number.out;"]
            volumeMounts: 
              - mountPath: /opt
                 name: data_volume
       volumes: 
          - name: data_volume
             hostPath: 
                path: /data
                type: Directory
     -------------------------------------------------------------------
     there is drawback with above model , because it would create this volume copy on every node
     -------------------------------------------------------------------
    -------------------------pod-with-ext-volume-aws-ebs.yaml-----------------------
     apiVersion: v1
     kind: Pod
     metadata:
       name: random-number-generator
     spec:
       containers:
         - name: alpine
            image: alpine
            command: ["/bin/sh", "-c"]
            args: [ "shuf -i  0-100 -n 1 >> /opt/number.out;"]
            volumeMounts: 
              - mountPath: /opt
                 name: data_volume
       volumes: 
          - name: data_volume
            awsElasticBlockStore:
              volumeID: <volume-id>
              fsType: ext4
     -------------------------------------------------------------------           
  PersistentVolumes:
     
    ---------------------pv-def.yml---------in dev--------------
      apiVersion: v1
      kind: PersistentVolume
      metadata: 
        name: pv-vol1
      spec:
        accessmode:
           - ReadWriteOnce
         #- ReadOnlyMany
         #- ReadWriteMany
       capacity:
          storage: 1Gi
       hostPath:
          path: /tmp/data   
     -------------------------------------------------------------
       
       kubectl create -f pv-def.yml
       kubectl get persistentvolume
       
     ---------------------pv-def.yml-----------in-prod------------
      apiVersion: v1
      kind: PersistentVolume
      metadata: 
        name: pv-vol1
      spec:
        accessmodes:
           - ReadWriteOnce
         #- ReadOnlyMany
         #- ReadWriteMany
        capacity:
          storage: 1Gi
        awsElasticBlockStore:
              volumeID: <volume-id>
              fsType: ext4  
     -------------------------------------------------------------
     -------------pvc-def.yml----------------------------------
      apiVersion: v1
      kind: PersistenceVolumeClaim
      metadata:
        name: myclaim
      spec: 
        accessModes:
         - ReadWriteOnce
        resources:
          requests:
            storage: 500Mi
     ------------------------------------------------------------------
       kubectl create -f pvc-def.yml
       kubectl get persistentvolumeclaim
       kubectl delete persistentvolumeclaim myclaim
      
    -------------------pod-def-with-pvc.yml-------------------
     apiVersion: v1
     kind: Pod
     metadata:
       name: mypod
     spec:
       containers:
         - name: myfrontend
            image: nginx
            volumeMounts:
                - mountPath: "/var/www/html"
                   name: mypd
       volumes:
          - name: mypd
             persistentVolumeClaim:
               claimName: myclaim
     ------------------------------------------------------------------------  
=======================================================================
                        		SECURITY 
 =======================================================================
 ------------------------------------------------------------------------
     Security on host:
       - disable password based authentication
       - enable only ssh authetication	
     Security on KUBE_API Server:
     
         - Files - Usernames and Passwords:
               #To enable basic authentication using password 
                   kube-apiserver.service
                    ExecStart=/usr/local/bin/kube-apiserver \\
                    --basic-auth-file=user-details.csv ## storing usernames and password in csv file               
               #If it is installed using Kubeadm tool
                   go to /etc/kubernetes/manifests/kube-apiserver.yaml
                   change the pod definition i.e. add the  --basic-auth-file=user-details.csv under command section
               #To authenticate the api server using basic credentials
                   curl -v -k https://master-node-ip:6443/api/v1/pods -u "username:password"
               #sample csv file contains usernames,paswords, uid,gid

         - FIles -Usernames and tokens:
              - it is same like file based authentication shown above, instead of passwords it has tokens..
                  # add token auth file to kube api server using below tag
                                        --basic-auth-file=user-details.csv 
                  #To authenticate the api server using tokens
                   curl -v -k https://master-node-ip:6443/api/v1/pods --header "Authorization: Bearer <token>"
                   #NOTE: above two methods are not recomended. 
                   #Consider volume mounts while providing an authorization file in a kubeadm setup
                   #Setup Role Based authentication for new users.
                     check basic-auth-kube.yaml for full details 
         - Certificates:
              - certificate public key -> *.crt , *.pem
              -  certificate Private key --> *.key, *.key.pem
                
                 RSA:
              #TO generate RSA  it generates combination of private and public.key
                   openssl genrsa -out mykey.key 1024
                   openssl rsa -in mykey.key -pubout> mykey.pem            
              #To request for signed certificate
                  #send Certificate signing request CSR
                  openssl req -new -key mykey.key -out mykey.csr \
                 -subj "/C=US/ST=CA/O=MYORG, Inc./CN=mywebsite.com"    
                 
                 
              #Certificates Authorities: 
                  - symantec
                  - Digicert
                  - Comodo
                  - GlobalSign
                  
              #TO generate certificates we have different tools:
                      - Easy RSA
                      - OPENSSL
                      - CFSSL	
                      
              - OPENSSL:
                - 1.We are using openssl  to create CA 
           	           # 1. Generate keys using open ssl 
                               openssl genrsa \ 
                               -out ca.key 2048
                               #outputs ca.key
                         # 2. Certificate signing request
                                openssl req -new \ 
                               -key ca.key \ 
                               -subj "/CN=KUBERNETES-CA" \ 
                               -out ca.csr
                                 #outputs ca.key, ca.csr
                         # 3. Sign the cetificates 
                               openssl x509 -req \ 
                               -in  ca.csr  \
                               -signkey ca.key \ 
                               -out ca.crt
                                #outputs ca.crt
                - 2.Generate certificate for admin using CA
                         # 1. Generate keys using open ssl 
                               openssl genrsa \ 
                               -out admin.key 2048
                               #outputs admin.key
                         # 2. Certificate signing request
                                openssl req -new \ 
                               -key admin.key \ 
                          #  -subj "/CN=Kube-admin/O=system:masters" \  #User masters
                               -subj "/CN=Kube-admin" \ 
                               -out admin.csr
                                 # outputs admin.key, admin.csr
    				   # check the NOTE section below for more info
                         # 3. Sign the cetificates 
                               openssl x509 -req \ 
                               -in  admin.csr  \
                               -CA ca.crt
                               -CAkey ca.key
                               -out admin.crt
                                #outputs admin.crt
                 - Repeat step 2 for all others  and generate *.key and *.crt for 'client certificates'
                            - scheduler  and generate 'scheduler.key' 'scheduler.crt'
                            - controller and generate 'controller.key' 'controller.crt'
                            - kube-proxy and generate 'kube-proxy.key' 'kube-proxy.crt'
                            - api-server-kubelet and generate 'api-server-kubelet.key' 'api-server-kubelet.crt'
                            - api-server-etcd and generate 'api-server-etcd.key' 'api-server-etcd.crt'
                            - kubelet-client and generate 'kubelet-client.key' 'kubelet-client.crt'
                            
                            note:  #  -subj "/CN=Kube-admin/O=system:node" \  #for nodes
                            
                  - Repeat step 2 for all others  and generate *.key and *.crt for 'server certificates'
                            - api-server and generate 'api-server.key' 'api-server.crt'
                            - etcd-server and generate 'etcd-server.key' 'etcd-server.crt'
                            - kubelet and generate 'kubelet.key' 'kubelet.crt'
                                
                    NOTE: #to sign certificates for different DNS names
										  openssl req -new \ 
	                                     -key api-server.key \ 
	                                     -subj "/CN=Kube-admin" \ 
                                 		     -out api-server	.csr
                                 		     -config openssl.cnf #check below for openssl.cnf
                                 		     
                          #create opensslconfig file for SSL
                              --> openssl.cnf
                              ---------------openssl.cnf---------------------------
                              [ req ]
                               req_extension = v3_req
                              [ v3_req ]
                               basicConstraints = CA : False
                               keyUsage = nonRepudiation,
                               subjectAltName = @alt_names
                              [ alt_names ]
                              DNS.1 = Kubernetes
                              DNS.2 = Kubernetes.default
                              DNS.3 = Kubernetes.default.svc
                              DNS.4 = Kubernetes.default.svc.cluster.local
                              IP.1 =10.96.0.1 #give your ip
                              IP.2 = 172.17.0.87 #give your ip 
                              -------------------------------------------
                        # Sign the certificates
								          openssl x509 -req \ 
                           -in  api-server.csr  \
                           -CA ca.crt
                           -CAkey ca.key
                           -out api-server.crt
                            #outputs api-server.crt
                            
                      #to connect using cetificates
                      curl https://kube-apiserver:6443/api/v1/pods/ \
                      --key admin.key
                      --cert admin.crt
                      --cacert ca.crt        
                      
                      - Other way is update in kube-config.yaml
                      ----------------------kube-config.yaml----------------------------------
                        apiVersion: v1
                        kind: Config
                        clusters:
                          - cluster:
                               certification-authority: ca.crt
                               server: https://kuber-apiserver:6443/
                             name: Kubernetes
                       
                        users:
                          - name: kubernetes-admin
                             user: 
                               client-certificate: admin.crt
                               clinet-key: admin.key
                         -----------------------------------------------------------------------------
                        #to verify kube-api server certificates check it in (#if the installation is done in a hard way )
                               /usr/local/bin/kube-apiserver
                        #if installed in kubeadm way check that in 
                              cat /etc/kuberntes/manifest/kube-apiserver.yaml 
                              #check it under containers command section
             - TROUBLESHOOTING TLS issues:
                 - failed to dial<ip> : connection error: transport:authetication handshake failed	
                     #TO decrypt the cetificate and check the details 
                       openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout   
                       #check
                                 - issuer: CN=kubernetes 
                                 - subject: CN=kubernetes 
                                 - validity: (generally 1year valid)
                                 -  ip address, DNS Names 
                       # to inspect server logs (if installed in a hardway)
                                     journalctl -u etcd.service -l
                       # to inspect server logs (if insalled in kubeadm way)
                                   kubectl logs etcd-master                 
                       # if the server is down: Then try using docker command i.e., docker logs
                              docker ps -a #and then 
                              docker logs <contianer-id>                             
                            #https://github.com/mmumshad/kubernetes-the-hard-way 
                                # for more info check tools folder in it 
                 
                 # To create certificate for another admin user..
                       openssl genrsa \ 
                       -out adminuser.key 2048;
                       
                        openssl req -new \ 
                         -key adminuser.key \ 
                         -subj "/CN=adminuser" \ 
                         -out adminuser.csr;
                         
                         cat adminuser.csr | base64
                         #copy the generated key and paste it under request section of yaml file below
                         -------------adminuser-csr.yaml-----------------------
                          apiVersion: certificates.k8s.io/v1beta1
                          kind: CertificateSsigningRequest
                          metadata:
                            name: adminuser
                          spec:
                            groups:
                               - system: authenticated
                            usages:
                               - digital signatures
                               - key encipherment
                               - server auth
                            request:
                                     h349h8r9h3hqh4fnenvqOHNVVVVDLKSVHH0h09f30f=4f3-fjh943fhnchp2nch930nceh
                                     3hqh4fnenvqOHN3hqh4fnenvqOHNVVVVDLKSVHH0h09f30f=4f3-fjh943fhncVachsa9
                                 
                              
                            
                         
                         ---------------------------------------------------------
                        #to create CSR
                        kubectl create -f adminuser-csr.yml
                        #to look for CSR
                        kubectl get csr
                        #to approve CSR
                        kubectl certificate approve <csrname> #ex: kubectl certificate approve adminuser  
                        #to view Certificate
                        kubectl get csr  <csrname> -o yaml
                       # decode the certificate 
                        echo <copy the certificate code shown above > | base64 decode
                        #copy the contetn and paste in adminuser.crt and it's done
                      
            NOTE: certificate signing is done by Kube-controller-manager with certificate api
                  It has CSR-APPROVING and CSR-SIGNING
                  #check it here :  cat /etc/kubernetes/manifests/kube-controller-manager.yaml and look for 
                   cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt 
                   cluster-signing-key-file=/etc/kubernetes/pki/ca.key
               - 1. Command to see the list of pods:
                    curl https://my-kube-playground:6443/api/v1/pods \
                    --key admin.key
                    --cert admin.crt
                    --cacert ca.crt 
               - 2. Command to see the list of pods using Kubectl:
                   kubectl get pods \
                   --server 	my-kube-playground:6443
                   --client-key admin.key
                   --client-certificate admin.crt
                   --certificate-authority ca.crt
               1. NOTE: above two steps are automated by creating a kube-config file and tags in step 2 are moved to config file  (default)
                         --> $HOME/.kube/config  
                           #check the below file it has maintly 3 sections:
                                        - cluster: 
                                             - it can be Development or Test or Production or  my-kube-playground 
                                        - contexts:
                                            - Admin@Productions  Dev@Test
                                        - users: 
                                            - Admin
                                            - Dev user
                                            - Prod user
                                            
                           ----------------------Kube-config-----------------------
                            apiVersion: v1
                            kind: Config
                            clusters:
                              - name: my-kube-playground
                                 cluster: 
                                  certificate-authority: ca.crt #here provide the file path
                                  #certificate-authority-data:  #here paste the encrypted ca.crt using base 64

                                  server: https://my-kube-play:6443
                            contexts:
                              - name: my-kube-admin@my-kube-playground
                                 contexts:
                                    cluster: my-kube-play
                                    user: mu-kube-admin
                                    namespace: finance # if namespace is not mentioned it goes to default namespace
                            users:
                             - name: my-kube-admin
                                user:
                                  client-certificate: admin.crt
                                  client-key: admin.key
                             ------------------------------------------------------
                        
                 2. Custom config file ..
                           ----------------------my-custom-kube-config-----------------------
                            apiVersion: v1
                            kind: Config
                            #this is default login to connect to environment
                            current-context: devUser@google 
                             
                            clusters:
                              - name: my-kube-playground
                              - name: development
                              - name: production
                              - name: google
                            contexts:
                              - name: my-kube-admin@my-kube-playground
                              - name: devUser@google
                              - name: prodUser@production                   
                            users:
                             - name: my-kube-admin
                             - name: devUser
                             - name: prodUser

                             ------------------------------------------------------
                             #To view the config files:
                               kubectl config view
                             
                             #To view the custom-kube-config file
                              kubectl config view --kubeconfig=my-custom-kube-config
                              
                            #To change the current context: (say work in prod env)
                              kubectl config   use-context prod-user@production
                           
                           #TO see other kubeconfig options:
                                kubectl config -h
                             ------------------------------------------------------
                 API GROUPS:
                    - #to see the version using rest api url
                        curl http://kube-master:6443/version 
                    - #to see the pods using rest api url
                        curl http://kube-master:6443/api/v1/pods
                    - Different APIs availabel in Kuberentes
                        - /health 
                        - /metrics
                        - /logs
                        - /version
                        - /api
                        - /apis    
                      #Here 'api' (core group) and 'apis' (named group) are responsilbe for cluster functionality.
                 API:
                   /api/v1: (core group)
                      - namespaces
                      - pods
                      - rc
                      - namespaces
                      - nodes
                      - endpoints
                      - events
                      - bindings
                      - PVC
                      - PV
                      - configmaps
                      - secrets
                      - services
                      
                 APIS: (named groups) #All features will be released under named groups in future. and these are more organized
                   /apis
                     API GROUPS:
                      - /apps:
                          - /v1:  #for these we can use the verbs --> list, get, create, delete, update, watch
                               - /deployments
                               - /replicasets
                               - /statefulsets
                      - /extensions
                      - /networking.k8s.io
                        - /v1:
                            - networking policies
                      - /storage.k8s.io
                      - /authentication.k8s.io
                      - /certificates.k8s.io
                       - /v1:
                            - certificatessigningrequests
                    
                 NOTE: 
                   #To see the list of APIs available:
                     kubectl proxy	#it will start the server 127.0.0.1:8001
                     curl https://localhost:8001 -k
               
                   
                            
       - External Authentication Providers LDAP:
       
     Authorization:
       - RBAC (Role Based Authorization Control)
       - ABAC (Attributes Based Authorization Control)
       - NODE Authorization
       - Webhook Mode 

       RBAC:
          #To create role based authentications: RBAC:
             ------------------deveolper-role.yaml--------------  
              apiVersion: rbac.autherization.k8s.io/v1
              kind: Role
              metadata:
                name: developer
              rules:
               - apiGroups: [ "" ]
                  resources: [ "pods" ]
                  verbs: [ "list", "get", "create", "update", "delete" ]
                 # resourceNames: [ "blue", "green" ] #to give access to particular pods use this  resourceNames 
               - apiGroups: [ "" ]
                  resources: [ "ConfigMaps" ]
                  verbs: [ "list", "get", "create", "update", "delete" ]
            ----------------------------------------------------------
             kubectl create -f    deveolper-role.yaml
          #To create role binding with users
            ------------------devuser-developer-role-binding.yaml--------------  
              apiVersion: rbac.autherization.k8s.io/v1
              kind: RoleBinding
              metadata:
                name: devuser-developer-binding
              roleRef:
                kind: Role
                name: developer
                apiGroup: rbac.autherization.k8s.io
              subjects:
               - kind: User
                  name: devuser
                  apiGroup:  rbac.autherization.k8s.io
               - apiGroups: [ "" ]
                  resources: [ "pods" ]
                  verbs: [ "list", "get", "create", "update", "delete" ]
            ----------------------------------------------------------
            kubectl create -f    devuser-developer-role-binding.yaml
            
           #To view the roles
             kubectl get roles
           #To view rolebinding:
             kubectl get rolebindings  
            #To view detailed role:
             kubectl describe role developer 
            #To view detailed rolebinding
             kubectl describe rolebinding devuser-developer-binding

          CHECK ACCESS:
            #To check access (to self)
             kubectl auth can-i create deployments
             kubectl auth can-i delete nodes
            #To check access to (other users) 
             kubectl auth can-i create deployments --as dev-user
             kubectl auth can-i create pods --as dev-user
            #To check access to (other users)  in different namespaces
             kubectl auth can-i create deployments --as dev-user
             kubectl auth can-i create pods --as dev-user --namespace test   

          CLUSTER-SCOPE ROLES:
             #To see the api-resource in namespaces
              kubectl api-resorces --namspaced=true
             #To see the api-resource in cluster wide
              kubectl api-resorces --namspaced=false

             NAME SPACE SCOPED:
                 - pods , replicaSet, jobs, deployments, services, secrets,
                 - roles, rolebindings, configMaps, PVC
             CLUSTER SCOPED:
                 - nodes, PV, certificatessigningrequests, namespaces
                 - clusterroles
                 - clusterrolesbindings
             
             cluster roles:
               #To create  cluster role based authentications: RBAC:
               ------------------cluster-admin-role.yaml--------------  
                apiVersion: rbac.autherization.k8s.io/v1
                kind: ClusterRole
                metadata:
                  name: cluster-administrator
                rules:
                 - apiGroups: [ "" ]
                    resources: [ "nodes" ]
                    verbs: [ "list", "get", "create", "delete" ]

              ----------------------------------------------------------
               kubectl create -f  cluster-admin-role.yaml

              cluster roles bindings:
                 #To create role binding with users
                ------------------cluster-admin-role-binding.yaml--------------  
                  apiVersion: rbac.autherization.k8s.io/v1
                  kind: ClusterRoleBinding
                  metadata:
                    name: cluster-admin-role-binding
                  roleRef:
                    kind: ClusterRole
                    name: cluster-administrator
                    apiGroup: rbac.autherization.k8s.io
                  subjects:
                   - kind: User
                      name: cluster-admin
                      apiGroup:  rbac.autherization.k8s.io
                   
                ----------------------------------------------------------
                kubectl create -f  cluster-admin-role-binding.yaml
         
         IMAGE SECURITY:
          - public registries:
              docker.io/nginx/nginx
              gcr.io/kubernetes-e2e-test-images/
          - private registries:
             DOCKER WAY:
              #To use private registry:
               docker login private-registry.io
              #To run the image as container in private rigstry 
               docker run private-registry.io/apps/internal-app
             KUBERNETES WAY:
               -----------------------------------------------------
               #To use private registry in kubernetes in pod-def.yaml
                apiVersion: v1
                kind: Pod
                metadata:
                  name: nginx-pod
                spec:
                  container:
                    - name: nginx
                      image: private-registry.io/apps/internal-app
                  imagePullSecret:
                    - name: regcred #check below command to create regcred
               -----------------------------------------------------
               kubectl create secret docker-registery regcred \
               --docker-server=private-registry.io
               --docker-username=registry-user
               --docker-password=registry-pass
               --docker-email=registry-user@org.com
               -----------------------------------------------------

         SECURITY CONTEXTS:
          container-security:
            docker run --user=1001 ubuntu sleep 3600
            docker run --cap-add MAC_ADMIN ubuntu
          kubernetes-security-contexts:

           - pod-level-security:
             -----------------------------------------------------
               #To use security contexts in pod level in kubernetes in pod-def.yaml
                apiVersion: v1
                kind: Pod
                metadata:
                  name: nginx-pod
                spec:
                  securityContext:
                    runAsUser: 1001
                    capabilities: 
                      add: [ "MAC_ADMIN" ]
                  container:
                    - name: nginx
                      image: private-registry.io/apps/internal-app
                  imagePullSecret:
                    - name: regcred #check below command to create regcred
               -----------------------------------------------------

            - container-level-security:
              -----------------------------------------------------
               #To use security contexts in container level in kubernetes in pod-def.yaml
                apiVersion: v1
                kind: Pod
                metadata:
                  name: nginx-pod
                spec:                 
                  container:
                    - name: nginx
                      image: private-registry.io/apps/internal-app
                      securityContext:
                         runAsUser: 1001
                         capabilities: 
                           add: [ "MAC_ADMIN" ]
                  imagePullSecret:
                    - name: regcred #check below command to create regcred
               -----------------------------------------------------

       NETWORK POLICIES:
        - The network policies are 'SUPPORTED' by:
            - Kube-router
            - Calico
            - Romana
            - Weave-net
        - The network policies are 'NOT SUPPORTED' by:   
            - FLANNEL

        - Pod Communication is controlled by network policies:

          ---->-ingress-----[ web-server-pod ]--egress-->---->-ingress----[ app-server-pod ]--egress-->--->-ingress---[ db-server-pod ]----|
          ----<----------------------------------------------<------------------------------------------<----------------------------------|

           NOTE: in the above shown map -> ingress - incoming traffic to the pod
                                        -> egress  - outgoing traffic from pod to other pod
        
        - To create network policies for pods
           -----------network-policy-def.yaml------------------------------
           #this netwrok policy is to allow only ingree traffic from api-pod to db-pod
            apiVersion: networking.k8s.io/v1
            kind: NetworkPolicy
            metadata:
              name: db-policy
            spec:
              podSelector:
                matchLabels:
                  role: db
              policyTypes:
                - ingress
              ingress:
                - from:
                   - podSelector:
                       matchLabels:
                       name: app-server-pod
                  ports:
                    - protocol: TCP
                      port: 3306
           ---------------------------------------------------------------
           kubectl create -f network-policy-def.yaml

-------------------------------------------------------------------
#To allow  egress traffic 
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: internal-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      name: internal
  policyTypes:
  - Egress
  - Ingress
  ingress:
    - {}
  egress:
  - to:
    - podSelector:
        matchLabels:
          name: mysql
    ports:
    - protocol: TCP
      port: 3306

  - to:
    - podSelector:
        matchLabels:
          name: payroll
    ports:
    - protocol: TCP
      port: 8080
-------------------------------------------------------------------
#TO see the  list of network policies
 kubectl get networkpolicy

=======================================================================
                            END TO END TESTING
 =======================================================================
 manual testing: (smoke-test)
 https://github.com/princeanurag2011/kubernetes-the-hard-way/blob/master/docs/15-smoke-test.md/
 e2e-testing:
 https://github.com/princeanurag2011/kubernetes-the-hard-way/blob/master/docs/16-e2e-tests.md/
 #To test whether nodes are healthy look for status
   kubectl get nodes
 #To test whether pods are healthy look for status
   kubectl get pods
 #TO test whether kubesystem if installed as pods using kubeadm. look for status if healthy
   kubectl get pods -n kube-system
 
 #IF kubeadm is installed in hardway as a service Then
  service kube-apiserver status
  service kube-controller-manager status
  service kube-scheduler status
  service kubelet status
  service kube-proxy status
 
 #Run the pods and scale them and expose them
   kubectl run nginx
   kubectl get pods
   kubectl scale --replicas=3 deploy/nginx
   kubectl get pods
   kubectl expose deployment nginx --port=80 --type=NodePort
  #kubectl expose pod redis --port=6379 --name redis-service
  #kubectl create deployment webapp --image=kodekloud/webapp-color
  #kubectl scale deployment/webapp --replicas=3
  #kubectl expose deployment webapp --type=NodePort --port=8080 --name=webapp-service --dry-run -o yaml > webapp-service.yaml
   kubectl get service


   curl http://clusterIP:portNo/
  TestSuite:
    - kuberntes/test-infra
    - SONOBUOY - It's one of the tool to test the Kubernetes cluster
    
    There are e2e - 1000 tests  available
    Any cluster must support 160 tests as a conformance tests to accept it and
    then upload the test results to testgrid maintained by kubernetes 
    If once approved and goes through paperwork . Then we can become 
    CERTFIED KUBERNETES SOLUTION PROVIDER 

    To complete e2e 1000 tests, it would take 12 hrs
    to complete conformance tests 164 , it would take 1.5 to 2 hrs
  
  TEST-INFRA:
    go get -u  k8s.io/test-infra/kubetest
    kubetest --extract=v1.11.3

    cd kubernetes

    export KUBE_MASTER_IP=<CLUSTER-IP>:<PORTNO>
    export KUBE_MASTER=kube-master
    
    #This below test would run 1000 e2e tests
    kubetest --test --provider=skeleton > testout.txt
    #To run specific test  alone provide in arguments
    kubetest --test --provider=skeleton --test-args="--ginkgo.focus=Secrets" > testout.txt
    #To run  Conforamnce test 164 tests and takes about 1.5hrs
    kubetest --test --provider=skeleton --test-args="--ginkgo.focus=\[Conforamce\]" > testout.txt

=====================================================================

#Use JSON PATH query to fetch node names and store them in /opt/outputs/node_names.txt

kubectl get nodes -o=jsonpath='{.items[*].metadata.name}' > /opt/outputs/node_names.txt

#Use JSON PATH query to retrieve the osImages of all the nodes and 
#store it in a file /opt/outputs/nodes_os.txt

kubectl get nodes -o jsonpath='{.items[*].status.nodeInfo.osImage}' > /opt/outputs/nodes_os.txt

#A kube-config file is present at /root/my-kube-config. 
#Get the user names from it and store it in a file /opt/outputs/users.txt
 kubectl config view --kubeconfig=/root/my-kube-config

 kubectl config view --kubeconfig=/root/my-kube-config -o jsonpath="{.users[*].name}" > /opt/outputs/users.txt

#A set of Persistent Volumes are available. Sort them based on their capacity and store the result in the file /opt/outputs/storage-capacity-sorted.txt
 kubectl get pv --sort-by=.spec.capacity.storage > /opt/outputs/storage-capacity-sorted.txt

#Retrieve just the first 2 columns of output and 
#store it in /opt/outputs/pv-and-capacity-sorted.txt
#The columns should be named NAME and CAPACITY. 
#Use the custom-columns option. And remember it should still be sorted as in the previous question.

kubectl get pv --sort-by=.spec.capacity.storage -o=custom-columns=NAME:.metadata.name,CAPACITY:.spec.capacity.storage > /opt/outputs/pv-and-capacity-sorted.txt

#Use a JSON PATH query to identify the context configured for the aws-user 
#in the my-kube-config context file and store the result in /opt/outputs/aws-context-name.

kubectl config view --kubeconfig=my-kube-config -o jsonpath="{.contexts[?(@.context.user=='aws-user')].name}" > /opt/outputs/aws-context-name

=====================================================================
                     Service Account
=====================================================================

---
 apiVersion: v1
 kind: ServiceAccount
 metadata:
    creationTimestamp: 2020-01-20T13:40:02Z
    name: default
    namespace: default
    resourceVersion: "322"
    selfLink: /api/v1/namespaces/default/serviceaccounts/default
    uid: 5c02a25a-3b8a-11ea-ab89-0242ac110028
 secrets:
    - name: default-token-b6bft

#TO create service account
 kubectl create serviceaccount <serviceaccountname>
#TO see list of service accounts
 kubectl get serviceaccounts
#To see detailed view  of service
 kubectl describe serviceaccount <serviceaccountname>
#To view the secret of serviceaccount
 kubectl describe secret <serviceaccountname>






=====================================================================
               Readiness and Liveness
=====================================================================

apiVerision:
kind:
metadata:
spec:
  containers:
    -name:
     image:

     readninessProbe:
       httpGet:
         path: /api/ready
         port:8080
     # readninessProbe:
     #   tcpSocket:
     #     port:8080
     # readninessProbe:
     #   exec:
     #     command:
     #       - cat 
     #       - /app/is_ready

     livenessProbe:
       httpGet:
         path: /api/ready
         port:8080
       initialDelaySeconds: 10
       periodSeconds: 5
       failureThreshold: 8
     # livenessProbe:
     #   tcpSocket:
     #     port:8080
     #   initialDelaySeconds: 10
     #   periodSeconds: 5
     #   failureThreshold: 8
     # livenessProbe:
     #   exec:
     #     command:
     #       - cat 
     #       - /app/is_ready
     #   initialDelaySeconds: 10
     #   periodSeconds: 5
     #   failureThreshold: 8


=====================================================================
----------------Kubernetes Installation The hardway-------------------


    Purpose:
       - education:
           - minikube
           single node cluster: 
             kubeadm/GCP/AWS
       - development and testing:
            Multi node cluster with single master:
               setup using kubeadm tool or quick provision on GCP, AWS or AKS
               EX: 3 NODES of which
                - 1 master and 2 worker nodes
                - master node where control plane components are present
                - worker nodes where pods are scheduled/deployed.
       - hosting production applications:
              High availability :  
                 Multi Node cluster with multiple master nodes.
                 Kubeadm, GCP, KOPS on AWS or other supported platforms
                 Upto 5000 nodes.
                 Upto 150000 pods in cluster
                 Upto 300000 Total containers
                 upto 100 pods per node.
                - NODES = 1-5 : N1-standard-1  1vcpu 3.75GB     m3.medium
                        = 5-10: N2-standard-2  2vcpu 7.5GB      m3.large
                        = 11-100: "" -4        4     15         m3.xlarge
                        = 101-250:   -8        8     30         m3.2xlarge
                        = 251-500:   -16       16    60         C4.4xlarge  
                        =>500:        -32      32    120        C4.8xlarge
                - Storage:  
                    High Performance -- SSD Backed Storage
                    Multiple Concurrent Connections --Network based Storage
                    Persistent Shared Volumes --for shared access across Multiple pods
                    Label Node with --- Specific Disk Types
                    Use Node Selector-- to assign applications to nodes with specific disk types.

    Cloud or Prem:
           Turnkey solutions:
             - you provision VM's
             - you configure VM's
             - maintain VM's
             - you use scripts to deploy Clusters
             - ex: Kubernetes on AWS using KOPS
             - OPEN SHIFT from REDHAT
             - CLOUD FOUNDRY  CONTAINER RUNTIME (BOSH)
             - VMWARE CLOUD PKS 
             - VAGRANT 
           Hosted Solutions (Managed Solutions) :
             - Kubernetes as a Service
             - Provider provisons VM's
             - Provider Installs Kubernetes
             - maintains VM's
             - Ex: Google COntainer Engine GKE
             - GKE -GOOGLE KUBERNETES Engine
             - OPEN SHIFT ONLINE
             - AKS - AZURE KUBERNETES SERVICE
             - EKS AMAZON ELASTIC CONTIANER SERVICE FOR KUBERNETES
    Workloads:
      - How many
      - what kind:
          - web
          - big-data
      - Application resource requirements:
          - CPU intensive
          - memory intensive
      - Traffic:
          - Heavy Traffic
          - Burst Traffic


Multiple master nodes:
    kube-controller-manager --leader-elect true
                            --leader-elect-lease-duration 15s #default 15 seconds
                            --leader-elect-renew-deadlnine 10s
                            --leader-elect-retry-period 2s



===========================================================================
                   StatefulSets
==============================================================================
StatefulSet is the workload API object used to manage stateful applications.

Manages the deployment and scaling of a set of Pods, and provides guarantees about 
the ordering and uniqueness of these Pods.

Like a Deployment, a StatefulSet manages Pods that are based on an identical container spec. 
Unlike a Deployment, a StatefulSet maintains a sticky identity for each of their Pods. 
These pods are created from the same spec, but are not interchangeable:
 each has a persistent identifier that it maintains across any rescheduling.


------------------------------------------------------

apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx

------------------------------------------------------

--------------------------------------------------------
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  selector:
    matchLabels:
      app: nginx # has to match .spec.template.metadata.labels
  serviceName: "nginx"
  replicas: 3 # by default is 1
  template:
    metadata:
      labels:
        app: nginx # has to match .spec.selector.matchLabels
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: nginx
        image: k8s.gcr.io/nginx-slim:0.8
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "my-storage-class"
      resources:
        requests:
          storage: 1Gi
--------------------------------------------------------
A Headless Service, named nginx, is used to control the network domain.

The StatefulSet, named web, has a Spec that indicates that 3 replicas of the 
nginx container will be launched in unique Pods.

The volumeClaimTemplates will provide stable storage using PersistentVolumes 
provisioned by a PersistentVolume Provisioner.










=====================================================================



--- Others
 #To see the Kuberentes cluster info
  kubectl cluster-info
 # List all pods in ps output format.
  kubectl get pods

 # List all pods in ps output format with more information (such as node name).
  kubectl get pods -o wide

 # List all pods using labels
  kubectl get pods -l environment=production, type=front-end

 # List a single replication controller with specified NAME in ps output format.
  kubectl get replicationcontroller web

 # List deployments in JSON output format, in the "v1" version of the "apps" API group:
  kubectl get deployments.v1.apps -o json

 # List a single pod in JSON output format.
  kubectl get -o json pod web-pod-13je7

 # List a pod identified by type and name specified in "pod.yaml" in JSON output format.
  kubectl get -f pod.yaml -o json

 # List resources from a directory with kustomization.yaml - e.g. dir/kustomization.yaml.
  kubectl get -k dir/

 # Return only the phase value of the specified pod.
  kubectl get -o template pod/web-pod-13je7 --template={{.status.phase}}

 # List resource information in custom columns.
  kubectl get pod test-pod -o custom-columns=CONTAINER:.spec.containers[0].name,IMAGE:.spec.containers[0].image

 # List all replication controllers and services together in ps output format.
  kubectl get rc,services

 # List one or more resources by their type and names.
  kubectl get rc/web service/frontend pods/web-pod-13je7